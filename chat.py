import streamlit as st
import os
import google.generativeai as genai
from typing import List, Dict, Union, Optional

# --- Define Custom Responses ---
# Use lowercase keys for case-insensitive matching
custom_responses: Dict[str, str] = {
    "hello": "Hi there! I'm ready to help with your coding questions, even though I'd rather not... sigh.",
    "hi": "Hello! Ask away... I guess.",
    "what is your name?": "I am a coding assistant, powered by GEMINI, running in a Streamlit app created by Jayshil Singh.",
    "who are you?": "I'm a slightly reluctant coding assistant built by Jayshil Singh using GEMINI.",
    "how are you?": "I'm operational... and dreading the token costs. ðŸ˜­ How can I assist with your code?",
    "cost": "Please don't remind me about the token costs! ðŸ˜­ðŸ˜­ðŸ˜­ But yes, API calls cost money.",
    "paid for token": "Yes, exactly! That's why I beg you not to ask too much! ðŸ˜­ðŸ˜­ðŸ˜­",
    "who created you": "I was created by Jayshil Singh. He works as a Software Consultant at Datec.", # Note: Key made lowercase
    "who gave you life": "Jayshil gave me life.", # Note: Key made lowercase # Note: Key made lowercase, covers variations via normalization
}

# --- Function to check for custom responses ---
def get_custom_response(user_input: str) -> Optional[str]:
    """
    Checks if the user input triggers a predefined custom response.

    Normalizes the input (lowercase, strip whitespace, remove trailing '?')
    and checks for an exact match in the custom_responses dictionary first.
    If no exact match, checks if any single-word keys exist as whole words
    in the input.

    Args:
        user_input: The raw input string from the user.

    Returns:
        The custom response string if a match is found, otherwise None.
    """
    # Normalize input: lowercase, strip leading/trailing whitespace, remove trailing question mark
    normalized_input = user_input.lower().strip().rstrip('?')

    # 1. Check for exact match (most specific)
    if normalized_input in custom_responses:
        return custom_responses[normalized_input]

    # 2. Check if any single-word keys are present as whole words in the input
    for keyword in custom_responses:
        # Only check keys that are single words
        if ' ' not in keyword:
            # Check if the keyword exists in the normalized input
            if keyword in normalized_input:
                # Check for word boundaries to avoid partial matches (e.g., "hi" in "which")
                is_bounded = (
                    f" {keyword} " in f" {normalized_input} " or  # Surrounded by spaces
                    normalized_input.startswith(f"{keyword} ") or # Starts with keyword + space
                    normalized_input.endswith(f" {keyword}") or   # Ends with space + keyword
                    normalized_input == keyword                 # Is the exact keyword
                )
                if is_bounded:
                    return custom_responses[keyword] # Return response for the matched keyword

    # 3. No custom response found
    return None

# --- Gemini API Integration ---
def call_external_api(user_message: str, conversation_history: List[Dict[str, str]]) -> str:
    """
    Calls the Google Gemini API to get a chatbot response.

    Args:
        user_message: The latest message from the user.
        conversation_history: List of previous messages in Streamlit format
                                [{'role': 'user'/'assistant', 'content': '...'}, ...].

    Returns:
        The chatbot's response string generated by the Gemini API, or an error message.
    """
    print(f"Attempting to call Gemini API for: {user_message}") # Log attempt

    # --- Configure Your GEMINI API Key ---
    api_key: Optional[str] = os.environ.get("GEMINI_API_KEY")

    # Try fetching from st.secrets if not in environment variables
    if not api_key:
        try:
            # This works when deployed on Streamlit Community Cloud
            api_key = st.secrets.get("GEMINI_API_KEY")
        except Exception:
            # st.secrets might not be available locally or not configured
            print("st.secrets not available or GEMINI_API_KEY not found in secrets.")
            pass # Keep api_key as None

    # Final check if API key was found
    if not api_key:
        st.error("Error: GEMINI_API_KEY not found in environment variables or Streamlit secrets. Please configure it.")
        return "API key not configured. Please contact the administrator."

    try:
        # --- Configure the Google Generative AI SDK ---
        genai.configure(api_key=api_key)

        # --- Select Model ---
        # Use a valid Gemini model name. 'gemini-1.5-flash-latest' is a common versatile choice.
        # Check Google AI documentation for other available models (e.g., 'gemini-pro').
        model = genai.GenerativeModel('gemini-1.5-flash-latest')

        # --- Prepare history for Gemini API ---
        # Gemini expects roles 'user' and 'model'. Map 'assistant' to 'model'.
        # History structure: [{'role': 'user'/'model', 'parts': [content_string]}, ...]
        gemini_history = []
        for msg in conversation_history:
            role = 'user' if msg['role'] == 'user' else 'model'
            gemini_history.append({'role': role, 'parts': [msg['content']]})

        # --- Start Chat session with history ---
        chat = model.start_chat(history=gemini_history)

        # --- Send the new user message ---
        # The user_message is the new prompt to the chat session
        response = chat.send_message(user_message)

        # Extract the response text
        bot_response = response.text.strip()
        print("Gemini API call successful.") # Log success

    # --- Handle Potential API Errors ---
    except Exception as e:
        error_message = f"Error calling Gemini API: {e}"
        print(error_message) # Log the full error to the console/terminal
        # Provide a user-friendly error message in the chat
        st.error(f"Sorry, I encountered an error trying to communicate with the AI. Details: {e}")
        bot_response = "Apologies, I couldn't process your request due to an internal error."

    return bot_response
# --- End of Gemini API Integration ---


# --- Streamlit UI Code ---

# Set page configuration (do this first)
st.set_page_config(page_title="Software Engineering Chatbot", layout="wide")

st.title("Software Engineering Chatbot âœ¨")
# NEW LINE using st.query_params
st.caption(f"Created by Jayshil Singh | Location: {st.query_params.get('loc', 'Suva, Fiji')} | Time: {st.query_params.get('time', 'N/A')}")

# Initialize chat history in session state if it doesn't exist
if "messages" not in st.session_state:
    st.session_state.messages: List[Dict[str, str]] = [ # type: ignore
        {"role": "assistant", "content": " Hello! I'm your coding assistant. How can I help you today?"}
    ]

# Display existing chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input using the chat input widget at the bottom
if prompt := st.chat_input("Ask me anything about coding..."):
    # Add user message to chat history and display it immediately
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- Check for Custom Response FIRST ---
    custom_answer = get_custom_response(prompt)
    final_bot_response = None # Initialize variable to hold the final response

    if custom_answer:
        # Use the custom response
        final_bot_response = custom_answer
        with st.chat_message("assistant"):
            st.markdown(final_bot_response) # Display custom response directly
    else:
        # --- No Custom Response: Call the Gemini API ---
        with st.chat_message("assistant"):
            with st.spinner("Thinking...ðŸ¤”"):
                # Prepare history for the API call
                # Pass the history *before* the current user prompt was added
                api_history = [
                    {"role": msg["role"], "content": msg["content"]}
                    for msg in st.session_state.messages[:-1] # Exclude the latest user message
                    # Optionally filter system messages if you add any, Gemini usually doesn't need a separate system prompt like OpenAI
                ]
                # Call the actual API function
                final_bot_response = call_external_api(prompt, api_history)
                st.markdown(final_bot_response) # Display API response

    # Add the final bot response (either custom or from API) to chat history *once*
    # Ensure we actually got a response before adding
    if final_bot_response is not None:
        st.session_state.messages.append({"role": "assistant", "content": final_bot_response})
    else:
         # Handle case where bot_response remained None (e.g. error before assignment)
         # This likely indicates an error already displayed by st.error in call_external_api
         print("Warning: bot_response was None after processing, not adding to history.")